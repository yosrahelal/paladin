// Copyright Â© 2025 Kaleido, Inc.
//
// SPDX-License-Identifier: Apache-2.0
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package perf

import (
	"context"
	"encoding/json"
	"fmt"
	"math"
	"os"
	"os/signal"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/google/uuid"
	"github.com/prometheus/client_golang/prometheus"

	"github.com/kaleido-io/paladin/perf/internal/conf"
	"github.com/kaleido-io/paladin/perf/internal/util"
	"github.com/kaleido-io/paladin/sdk/go/pkg/pldapi"
	"github.com/kaleido-io/paladin/sdk/go/pkg/pldclient"
	"github.com/kaleido-io/paladin/sdk/go/pkg/rpcclient"

	dto "github.com/prometheus/client_model/go"
	log "github.com/sirupsen/logrus"
	"golang.org/x/sync/errgroup"
	"golang.org/x/time/rate"
)

const workerPrefix = "worker-"

var METRICS_NAMESPACE = "pldperf"
var METRICS_SUBSYSTEM = "runner"

var totalActionsCounter = prometheus.NewGauge(prometheus.GaugeOpts{
	Namespace: METRICS_NAMESPACE,
	Name:      "actions_submitted_total",
	Subsystem: METRICS_SUBSYSTEM,
})

var receivedEventsCounter = prometheus.NewGauge(prometheus.GaugeOpts{
	Namespace: METRICS_NAMESPACE,
	Name:      "received_events_total",
	Subsystem: METRICS_SUBSYSTEM,
})

var incompleteEventsCounter = prometheus.NewGauge(prometheus.GaugeOpts{
	Namespace: METRICS_NAMESPACE,
	Name:      "incomplete_events_total",
	Subsystem: METRICS_SUBSYSTEM,
})

var delinquentMsgsCounter = prometheus.NewGauge(prometheus.GaugeOpts{
	Namespace: METRICS_NAMESPACE,
	Name:      "deliquent_msgs_total",
	Subsystem: METRICS_SUBSYSTEM,
})

var perfTestDurationHistogram = prometheus.NewHistogramVec(prometheus.HistogramOpts{
	Namespace: METRICS_NAMESPACE,
	Subsystem: METRICS_SUBSYSTEM,
	Name:      "perf_test_duration_seconds",
	Buckets:   []float64{1.0, 2.0, 5.0, 10.0, 30.0},
}, []string{"test"})

func Init() {
	prometheus.Register(delinquentMsgsCounter)
	prometheus.Register(receivedEventsCounter)
	prometheus.Register(incompleteEventsCounter)
	prometheus.Register(totalActionsCounter)
	prometheus.Register(perfTestDurationHistogram)
}

func getMetricVal(collector prometheus.Collector) float64 {
	collectorChannel := make(chan prometheus.Metric, 1)
	collector.Collect(collectorChannel)
	metric := dto.Metric{}
	err := (<-collectorChannel).Write(&metric)
	if err != nil {
		log.Errorf("error writing metric: %s", err)
	}
	if metric.Counter != nil {
		return *metric.Counter.Value
	} else if metric.Gauge != nil {
		return *metric.Gauge.Value
	}
	return 0
}

type PerfRunner interface {
	Init() error
	Start() error
}

type TestCase interface {
	WorkerID() int
	RunOnce(iterationCount int) (trackingID string, err error)
	Name() conf.TestName
	ActionsPerLoop() int
}

type inflightTest struct {
	time     time.Time
	testCase TestCase
}

type summary struct {
	mutex        *sync.Mutex
	rampSummary  int64
	totalSummary int64
}
type perfRunner struct {
	bfr        chan int
	cfg        *conf.RunnerConfig
	httpClient pldclient.PaladinClient
	wsClient   pldclient.PaladinWSClient
	ctx        context.Context
	shutdown   context.CancelFunc
	stopping   bool

	startTime     int64
	endSendTime   int64
	endTime       int64
	startRampTime int64
	endRampTime   int64

	totalWorkers  int
	reportBuilder *util.Report
	sendTime      *util.Latency
	receiveTime   *util.Latency
	totalTime     *util.Latency
	summary       summary
	msgTimeMap    sync.Map
	workerIDMap   sync.Map

	wsReceivers   map[string]chan bool
	subscriptions []rpcclient.Subscription
}

func New(config *conf.RunnerConfig, reportBuilder *util.Report) PerfRunner {
	if config.LogLevel != "" {
		if level, err := log.ParseLevel(config.LogLevel); err == nil {
			log.SetLevel(level)
		}
	}

	totalWorkers := 0
	for _, test := range config.Tests {
		totalWorkers += test.Workers
	}

	// Create channel based dispatch for workers
	wsReceivers := make(map[string]chan bool)
	for i := 0; i < totalWorkers; i++ {
		prefixedWorkerID := fmt.Sprintf("%s%d", workerPrefix, i)
		wsReceivers[prefixedWorkerID] = make(chan bool)
	}

	ctx, cancel := context.WithCancel(context.Background())

	startRampTime := time.Now().Unix()
	endRampTime := time.Now().Unix() + int64(config.RampLength.Seconds())
	startTime := endRampTime
	endTime := startTime + int64(config.Length.Seconds())

	pr := &perfRunner{
		bfr:           make(chan int, totalWorkers),
		cfg:           config,
		ctx:           ctx,
		shutdown:      cancel,
		startRampTime: startRampTime,
		endRampTime:   endRampTime,
		startTime:     startTime,
		endTime:       endTime,
		reportBuilder: reportBuilder,
		sendTime:      &util.Latency{},
		receiveTime:   &util.Latency{},
		totalTime:     &util.Latency{},
		msgTimeMap:    sync.Map{},
		workerIDMap:   sync.Map{},
		summary: summary{
			totalSummary: 0,
			mutex:        &sync.Mutex{},
		},
		wsReceivers:  wsReceivers,
		totalWorkers: totalWorkers,
	}
	return pr
}

func (pr *perfRunner) Init() (err error) {
	pr.httpClient, err = pldclient.New().HTTP(pr.ctx, &pr.cfg.HTTPConfig)
	if err != nil {
		return err
	}
	pr.wsClient, err = pr.httpClient.WebSocket(pr.ctx, &pr.cfg.WSConfig)
	if err != nil {
		return err
	}
	return nil
}

func (pr *perfRunner) Start() (err error) {
	log.Infof("Running test:\n%+v", pr.cfg)

	if containsTargetTest(pr.cfg.Tests, conf.PerfTestPublicContract) {
		if err = pr.subscribeToPublicContractListener(); err != nil {
			return err
		}
	}

	id := 0
	for _, test := range pr.cfg.Tests {
		log.Infof("Starting %d workers for case \"%s\"", test.Workers, test.Name)
		for iWorker := 0; iWorker < test.Workers; iWorker++ {
			var tc TestCase

			switch test.Name {
			case conf.PerfTestPublicContract:
				tc = newPublicContractTestWorker(pr, id, test.ActionsPerLoop)
			default:
				return fmt.Errorf("unknown test case '%s'", test.Name)
			}

			delayPerWorker := pr.cfg.RampLength / time.Duration(test.Workers)

			go func(i int) {
				// Delay the start of the next worker by (ramp time) / (number of workers)
				if delayPerWorker > 0 {
					time.Sleep(delayPerWorker * time.Duration(i))
					log.Infof("Ramping up. Starting next worker after waiting %v", delayPerWorker)
				}
				err := pr.runLoop(tc)
				if err != nil {
					log.Errorf("Worker %d failed: %s", tc.WorkerID(), err)
				}
			}(iWorker)
			id++
		}
	}

	signalCh := make(chan os.Signal, 1)
	signal.Notify(signalCh, os.Interrupt)
	signal.Notify(signalCh, os.Kill)
	signal.Notify(signalCh, syscall.SIGTERM)
	signal.Notify(signalCh, syscall.SIGQUIT)
	signal.Notify(signalCh, syscall.SIGKILL)

	i := 0
	lastCheckedTime := time.Now()

	rateLimiter := rate.NewLimiter(rate.Limit(math.MaxFloat64), math.MaxInt)

	if pr.cfg.MaxSubmissionsPerSecond > 0 {
		rateLimiter = rate.NewLimiter(rate.Limit(pr.cfg.MaxSubmissionsPerSecond), pr.cfg.MaxSubmissionsPerSecond)
	}
	log.Infof("Sending rate: %f per second with %d burst", rateLimiter.Limit(), rateLimiter.Burst())
perfLoop:
	for pr.IsDaemon() || time.Now().Unix() < pr.endTime {
		timeout := time.After(60 * time.Second)
		// If we've been given a maximum number of actions to perform, check if we're done
		if pr.cfg.MaxActions > 0 && int64(getMetricVal(totalActionsCounter)) >= pr.cfg.MaxActions {
			break perfLoop
		}

		select {
		case <-signalCh:
			break perfLoop
		case pr.bfr <- i:
			err = rateLimiter.Wait(pr.ctx)
			if err != nil {
				log.Panic(fmt.Errorf("rate limiter failed"))
				break perfLoop
			}
			i++
			if time.Since(lastCheckedTime).Seconds() > pr.cfg.MaxTimePerAction.Seconds() {
				if pr.detectDelinquentMsgs() && pr.cfg.DelinquentAction == conf.DelinquentActionExit {
					break perfLoop
				}
				lastCheckedTime = time.Now()
			}
		case <-timeout:
			if pr.detectDelinquentMsgs() && pr.cfg.DelinquentAction == conf.DelinquentActionExit {
				break perfLoop
			}
			lastCheckedTime = time.Now()
		case <-pr.ctx.Done():
			pr.cleanup()
			break perfLoop
		}

	}

	pr.stopping = true

	idleStart := time.Now()

	if pr.cfg.NoWaitSubmission {
		eventsCount := getMetricVal(receivedEventsCounter)
		submissionCount := getMetricVal(totalActionsCounter)
		log.Infof("<No wait submission mode> Wait for the event count %f to reach request sent count %f", eventsCount, submissionCount)
		for {
			newEventsCount := getMetricVal(receivedEventsCounter)
			if eventsCount < newEventsCount {
				// reset idle start time if there are new events
				idleStart = time.Now()
			}
			if newEventsCount == submissionCount {
				break
			} else if newEventsCount > submissionCount {
				log.Warnf("The number of events received %f is greater than the number of requests sent %f.", newEventsCount, submissionCount)
				break
			}

			// Check if more than 30 seconds has passed
			if time.Since(idleStart) > 30*time.Second {
				log.Errorf("The number of events received %f doesn't tally up to the number of requests sent %f after 30s idle time, total tally time: %s.", eventsCount, submissionCount, time.Since(time.Unix(pr.startTime, 0)))
				break
			}

			time.Sleep(time.Second * 1)
			log.Infof("<No wait submission mode> Wait for the event count %f to reach request sent count %f", newEventsCount, submissionCount)
			eventsCount = newEventsCount
		}
	}

	measuredActions := pr.summary.totalSummary
	measuredTime := time.Since(time.Unix(pr.startTime, 0))

	testNames := make([]string, len(pr.cfg.Tests))
	for i, t := range pr.cfg.Tests {
		testNames[i] = string(t.Name)
	}

	tps := util.GenerateTPS(measuredActions, pr.startTime, pr.endSendTime)
	pr.reportBuilder.AddTestRunMetrics(strings.Join(testNames, ","), measuredActions, measuredTime, tps, pr.totalTime)
	err = pr.reportBuilder.GenerateHTML()

	if err != nil {
		log.Errorf("failed to generate performance report: %+v", err)
	}

	// we sleep on shutdown / completion to allow for Prometheus metrics to be scraped one final time
	// After 30 seconds workers should be completed, so we check for delinquent messages
	// one last time so metrics are up-to-date
	log.Warn("Runner stopping in 5s")
	time.Sleep(5 * time.Second)
	pr.detectDelinquentMsgs()

	log.Info("Cleaning up")

	pr.cleanup()

	log.Info("Shutdown summary:")
	log.Infof(" - Prometheus metric received_events_total   = %f\n", getMetricVal(receivedEventsCounter))
	log.Infof(" - Prometheus metric incomplete_events_total = %f\n", getMetricVal(incompleteEventsCounter))
	log.Infof(" - Prometheus metric delinquent_msgs_total    = %f\n", getMetricVal(delinquentMsgsCounter))
	log.Infof(" - Prometheus metric actions_submitted_total = %f\n", getMetricVal(totalActionsCounter))
	log.Infof(" - Test duration: %s", measuredTime)
	log.Infof(" - Measured actions: %d", measuredActions)
	log.Infof(" - Measured send TPS: %2f", tps.SendRate)
	log.Infof(" - Measured throughput: %2f", tps.Throughput)
	log.Infof(" - Measured send duration: %s", pr.sendTime)
	if !pr.cfg.NoWaitSubmission {
		log.Infof(" - Measured event receiving duration: %s", pr.receiveTime)
	}
	log.Infof(" - Measured total duration: %s", pr.totalTime)

	return nil
}

func (pr *perfRunner) cleanup() {
	for _, sub := range pr.subscriptions {
		err := sub.Unsubscribe(pr.ctx)
		if err != nil {
			log.Errorf("Error unsubscribing from subscription: %s", err.Error())
		} else {
			log.Info("Successfully unsubscribed")
		}
	}
	pr.wsClient.Close()

}

func (pr *perfRunner) batchEventLoop(sub rpcclient.Subscription) (err error) {
	log.Info("Batch Event loop started")
	for {
		log.Trace("blocking until wsconn.Receive or ctx.Done()")
		select {
		// Wait to receive websocket event
		case subNotification, ok := <-sub.Notifications():
			if !ok {
				log.Errorf("Error receiving websocket")
				return
			}
			log.Trace("received from websocket")

			// Handle websocket event
			var batch pldapi.TransactionReceiptBatch
			json.Unmarshal(subNotification.GetResult(), &batch)

			if pr.cfg.LogEvents {
				log.Info("Batch: ", string(subNotification.GetResult()))
			}

			g, _ := errgroup.WithContext(pr.ctx)
			g.SetLimit(-1)

			for _, receipt := range batch.Receipts {
				thisReceipt := receipt
				g.Go(func() error {
					transactionID := thisReceipt.ID.String()
					v, ok := pr.workerIDMap.LoadAndDelete(transactionID)
					if !ok {
						// we cant apply reliable filters to ensure we're only getting back receipts
						// for transactions submitted in this test, so just log and skip any we don't
						// recognuse
						log.Warnf("No worker ID map entry for transaction id: %s", transactionID)
						return nil
					}
					workerID := v.(int)

					if pr.cfg.LogEvents {
						eventJSON, _ := json.Marshal(thisReceipt)
						log.Info("Event: ", string(eventJSON))
					}

					receivedEventsCounter.Inc()
					pr.recordCompletedAction()
					// Release worker so it can continue to its next task
					if !pr.stopping && !pr.cfg.NoWaitSubmission {
						if workerID >= 0 {
							prefixedWorkerID := fmt.Sprintf("%s%d", workerPrefix, workerID)
							// No need for locking as channel have built in support
							pr.wsReceivers[prefixedWorkerID] <- true
						}
					}
					return nil
				})
			}

			// Wait for all go routines to complete
			// The first non-nil go routine will be returned
			// and we will return the error
			log.Debug("Waiting for events from websocket to be handled")
			if err := g.Wait(); err != nil {
				return err
			}
			log.Debug("All events from websocket handled")

			// We have completed all the go routines
			// and can ack the batch
			err := subNotification.Ack(pr.ctx)
			if err != nil {
				log.Errorf("Failed to ack batch receipt: %s", err.Error())
				return err
			}

			pr.summary.mutex.Lock()
			pr.calculateCurrentTps(true)
			pr.summary.mutex.Unlock()
		case <-pr.ctx.Done():
			log.Warnf("Run loop exiting (context cancelled)")
			sub.Unsubscribe(pr.ctx)
			return
		}
	}
}

func (pr *perfRunner) allActionsComplete() bool {
	return pr.cfg.MaxActions > 0 && int64(getMetricVal(totalActionsCounter)) >= pr.cfg.MaxActions
}

func (pr *perfRunner) runLoop(tc TestCase) error {
	testName := tc.Name()
	workerID := tc.WorkerID()
	preFixedWorkerID := fmt.Sprintf("%s%d", workerPrefix, workerID)

	loop := 0
	for {
		select {
		case <-pr.bfr:
			var confirmationsPerAction int
			var actionsCompleted int

			// Worker sends its task
			hist, histErr := perfTestDurationHistogram.GetMetricWith(prometheus.Labels{
				"test": string(testName),
			})

			if histErr != nil {
				log.Errorf("Error retrieving histogram: %s", histErr)
			}

			startTime := time.Now()

			type ActionResponse struct {
				transactionID string
				err           error
			}

			actionResponses := make(chan *ActionResponse, tc.ActionsPerLoop())

			var sentTime time.Time
			var submissionSecondsPerLoop float64
			var eventReceivingSecondsPerLoop float64
			transactionIDs := make([]string, 0)

			for actionsCompleted = 0; actionsCompleted < tc.ActionsPerLoop(); actionsCompleted++ {

				if pr.allActionsComplete() {
					break
				}
				actionCount := actionsCompleted
				go func() {
					transactionID, err := tc.RunOnce(actionCount)
					log.Debugf("%d --> %s action %d sent after %f seconds", workerID, testName, actionCount, time.Since(startTime).Seconds())
					actionResponses <- &ActionResponse{
						transactionID: transactionID,
						err:           err,
					}
				}()
			}
			resultCount := 0
			for {
				aResponse := <-actionResponses
				resultCount++
				if aResponse.err != nil {
					if pr.cfg.DelinquentAction == conf.DelinquentActionExit {
						return aResponse.err
					} else {
						log.Errorf("Worker %d error running job (logging but continuing): %s", workerID, aResponse.err)
					}
				} else {
					transactionIDs = append(transactionIDs, aResponse.transactionID)
					pr.workerIDMap.Store(aResponse.transactionID, tc.WorkerID())
					pr.markTestInFlight(tc, aResponse.transactionID)
					log.Debugf("%d --> %s Sent transaction ID: %s", workerID, testName, aResponse.transactionID)
					totalActionsCounter.Inc()
				}
				// if we've reached the expected amount of metadata calls then stop
				if resultCount == tc.ActionsPerLoop() {
					submissionDurationPerLoop := time.Since(startTime)
					pr.sendTime.Record(submissionDurationPerLoop)
					submissionSecondsPerLoop = submissionDurationPerLoop.Seconds()
					sentTime = time.Now()
					log.Debugf("%d --> %s All actions sent %d after %f seconds", workerID, testName, resultCount, submissionSecondsPerLoop)

					pr.endSendTime = time.Now().Unix()
					break
				}
			}
			if pr.cfg.NoWaitSubmission {
				confirmationsPerAction = 0
			} else {
				confirmationsPerAction = 1
			}

			// Wait for worker to confirm the message before proceeding to next task

			for j := 0; j < actionsCompleted; j++ {
				var nextTransactionID string
				for i := 0; i < confirmationsPerAction; i++ {
					select {
					case <-pr.ctx.Done():
						return nil
					case <-pr.wsReceivers[preFixedWorkerID]:
						continue
					}
				}
				if len(transactionIDs) > 0 {
					nextTransactionID = transactionIDs[0]
					transactionIDs = transactionIDs[1:]
					pr.stopTrackingRequest(nextTransactionID)
				}
			}
			totalDurationPerLoop := time.Since(startTime)
			pr.totalTime.Record(totalDurationPerLoop)
			secondsPerLoop := totalDurationPerLoop.Seconds()

			if pr.cfg.NoWaitSubmission {
				log.Infof("%d <-- %s Finished (loop=%d) after %f seconds", workerID, testName, loop, secondsPerLoop)
			} else {
				eventReceivingDurationPerLoop := time.Since(sentTime)
				eventReceivingSecondsPerLoop = eventReceivingDurationPerLoop.Seconds()
				pr.receiveTime.Record(totalDurationPerLoop)

				total := submissionSecondsPerLoop + eventReceivingSecondsPerLoop
				subPortion := int((submissionSecondsPerLoop / total) * 100)
				envPortion := int((eventReceivingSecondsPerLoop / total) * 100)
				log.Infof("%d <-- %s Finished (loop=%d), submission time: %f s, event receive time: %f s. Ratio (%d/%d) after %f seconds", workerID, testName, loop, submissionSecondsPerLoop, eventReceivingSecondsPerLoop, subPortion, envPortion, secondsPerLoop)
			}

			if histErr == nil {
				log.Debugf("%d <-- %s Emmiting (loop=%d) after %f seconds", workerID, testName, loop, secondsPerLoop)

				hist.Observe(secondsPerLoop)
			}
			loop++
		case <-pr.ctx.Done():
			return nil
		}
	}
}

func containsTargetTest(tests []conf.TestCaseConfig, target conf.TestName) bool {
	for _, test := range tests {
		if test.Name == target {
			return true
		}
	}
	return false
}

func (pr *perfRunner) detectDelinquentMsgs() bool {
	delinquentMsgs := make(map[string]time.Time)
	pr.msgTimeMap.Range(func(k, v interface{}) bool {
		trackingID := k.(string)
		inflight := v.(*inflightTest)
		if time.Since(inflight.time).Seconds() > pr.cfg.MaxTimePerAction.Seconds() {
			delinquentMsgs[trackingID] = inflight.time
		}
		return true
	})

	dw, err := json.MarshalIndent(delinquentMsgs, "", "  ")
	if err != nil {
		log.Errorf("Error printing delinquent messages: %s", err)
		return len(delinquentMsgs) > 0
	}

	if len(delinquentMsgs) > 0 {
		log.Warnf("Delinquent Messages:\n%s", string(dw))
	}

	return len(delinquentMsgs) > 0
}

func (pr *perfRunner) markTestInFlight(tc TestCase, transactionID string) {
	if len(transactionID) > 0 {
		pr.msgTimeMap.Store(transactionID, &inflightTest{
			testCase: tc,
			time:     time.Now(),
		})
	}
}

func (pr *perfRunner) recordCompletedAction() {
	if pr.ramping() {
		_ = atomic.AddInt64(&pr.summary.rampSummary, 1)
	} else {
		_ = atomic.AddInt64(&pr.summary.totalSummary, 1)
	}
}

func (pr *perfRunner) stopTrackingRequest(transactionID string) {
	log.Debugf("Deleting tracking request: %s", transactionID)
	pr.msgTimeMap.Delete(transactionID)
}

func (pr *perfRunner) subscribeToPublicContractListener() error {
	// The listener is created outside of the test as if we create a new one per test run
	// we have to work out what sequence number to listen from. This has proved to be error
	// prone.
	listenerName := "publiclistener"
	sub, err := pr.wsClient.PTX().SubscribeReceipts(pr.ctx, listenerName)
	if err != nil {
		return err
	}

	pr.subscriptions = append(pr.subscriptions, sub)

	go pr.batchEventLoop(sub)

	return nil
}

func (pr *perfRunner) IsDaemon() bool {
	return pr.cfg.Daemon
}

func (pr *perfRunner) getIdempotencyKey(workerId int, iteration int) string {
	// Left pad worker ID to 5 digits (supporting up to 99,999 workers)
	workerIdStr := fmt.Sprintf("%05d", workerId)
	// Left pad iteration ID to 9 digits (supporting up to 999,999,999 iterations)
	iterationIdStr := fmt.Sprintf("%09d", iteration)
	return fmt.Sprintf("%v-%s-%s-%s", pr.startTime, workerIdStr, iterationIdStr, uuid.New())
}

func (pr *perfRunner) calculateCurrentTps(logValue bool) float64 {
	// If we're still ramping, give the current rate during the ramp
	// If we're done ramping, calculate TPS from the end of the ramp onward
	var startTime int64
	var measuredActions int64
	if pr.ramping() {
		measuredActions = pr.summary.rampSummary
		startTime = pr.startRampTime
	} else {
		measuredActions = pr.summary.totalSummary
		startTime = pr.startTime
	}
	duration := time.Since(time.Unix(startTime, 0)).Seconds()
	currentTps := float64(measuredActions) / duration
	if logValue {
		log.Infof("Current TPS: %v Measured Actions: %v Duration: %v", currentTps, measuredActions, duration)
	}
	return currentTps
}

func (pr *perfRunner) ramping() bool {
	if time.Now().Before(time.Unix(pr.endRampTime, 0)) {
		return true
	}
	return false
}
